{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "\n",
    "# def scrape_article_content(url):\n",
    "#     response = requests.get(url)\n",
    "#     response.raise_for_status()  # Check for request errors\n",
    "#     soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "#     # Find all the article sections\n",
    "#     main_content = soup.find('main')  # Main container for content\n",
    "#     articles = []\n",
    "\n",
    "#     if main_content:\n",
    "#         # Extract sections containing articles\n",
    "#         article_sections = main_content.find_all('h2')  # Assuming headings <h2> indicate sections\n",
    "\n",
    "#         for section in article_sections:\n",
    "#             title = section.get_text(strip=True)  # Extract title\n",
    "\n",
    "#             # Extract content paragraphs following the title\n",
    "#             paragraphs = []\n",
    "#             for sibling in section.find_next_siblings():\n",
    "#                 if sibling.name == 'h2':  # Stop when a new <h2> is encountered\n",
    "#                     break\n",
    "#                 if sibling.name == 'p':  # Extract paragraph content\n",
    "#                     paragraphs.append(sibling.get_text(strip=True))\n",
    "            \n",
    "#             content = \" \".join(paragraphs)\n",
    "#             if title and content:  # Include only non-empty content\n",
    "#                 articles.append({'Title': title, 'Content': content})\n",
    "\n",
    "#     return articles\n",
    "\n",
    "# def save_to_csv(data, file_name):\n",
    "#     # Remove rows with empty Title or Content\n",
    "#     clean_data = [row for row in data if row['Content'].strip()]\n",
    "#     df = pd.DataFrame(clean_data)\n",
    "#     df.to_csv(file_name, index=False)\n",
    "#     print(f\"Data saved to {file_name}\")\n",
    "\n",
    "# # URL of the webpage\n",
    "# url = \"https://www.health.harvard.edu/exercise-and-fitness/essential-stretches-to-fight-stiff-winter-muscles\"\n",
    "\n",
    "# # Scrape the data\n",
    "# scraped_data = scrape_article_content(url)\n",
    "\n",
    "# # Save the scraped data to CSV\n",
    "# save_to_csv(scraped_data, \"articles_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check for request errors\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all the article sections\n",
    "    main_content = soup.find('main')  # Main container for content\n",
    "    articles = []\n",
    "\n",
    "    if main_content:\n",
    "        # Extract sections containing articles\n",
    "        article_sections = main_content.find_all('h2')  # Assuming headings <h2> indicate sections\n",
    "\n",
    "        for section in article_sections:\n",
    "            title = section.get_text(strip=True)  # Extract title\n",
    "\n",
    "            # Extract content paragraphs following the title\n",
    "            paragraphs = []\n",
    "            for sibling in section.find_next_siblings():\n",
    "                if sibling.name == 'h2':  # Stop when a new <h2> is encountered\n",
    "                    break\n",
    "                if sibling.name == 'p':  # Extract paragraph content\n",
    "                    paragraphs.append(sibling.get_text(strip=True))\n",
    "            \n",
    "            content = \" \".join(paragraphs)\n",
    "            if title and content:  # Include only non-empty content\n",
    "                articles.append({'Title': title, 'Content': content})\n",
    "\n",
    "    return articles\n",
    "\n",
    "def save_to_csv(data, file_name):\n",
    "    # Remove rows with empty Title or Content\n",
    "    clean_data = [row for row in data if row['Content'].strip()]\n",
    "    df = pd.DataFrame(clean_data)\n",
    "    df.to_csv(file_name, index=False)\n",
    "    print(f\"Data saved to {file_name}\")\n",
    "\n",
    "# List of URLs to scrape\n",
    "urls = [\n",
    "    \"https://www.health.harvard.edu/exercise-and-fitness/essential-stretches-to-fight-stiff-winter-muscles\",\n",
    "    \"https://www.health.harvard.edu/topics/exercise-and-fitness\",\n",
    "    \"https://www.health.harvard.edu/diseases-and-conditions\",\n",
    "    # Add more links as needed\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store all scraped data\n",
    "all_articles = []\n",
    "\n",
    "# Loop through each URL and scrape data\n",
    "for url in urls:\n",
    "    print(f\"Scraping: {url}\")\n",
    "    scraped_data = scrape_article_content(url)\n",
    "    all_articles.extend(scraped_data)  # Append scraped data to the main list\n",
    "\n",
    "# Save all scraped data to a single CSV file\n",
    "save_to_csv(all_articles, \"articles_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Setup logging for better error handling\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all the article sections (main content area)\n",
    "        main_content = soup.find('main')  # Main container for content\n",
    "        articles = []\n",
    "\n",
    "        if main_content:\n",
    "            # Extract sections containing articles (assuming headings like h2, h3, h4 indicate sections)\n",
    "            article_sections = main_content.find_all(['h2', 'h3', 'h4'])  # Look for common section headings\n",
    "\n",
    "            for section in article_sections:\n",
    "                title = section.get_text(strip=True)  # Extract title\n",
    "\n",
    "                # Extract content paragraphs following the title\n",
    "                paragraphs = []\n",
    "                for sibling in section.find_next_siblings():\n",
    "                    if sibling.name in ['h2', 'h3', 'h4']:  # Stop when a new header is encountered\n",
    "                        break\n",
    "                    if sibling.name == 'p':  # Extract paragraph content\n",
    "                        paragraphs.append(sibling.get_text(strip=True))\n",
    "                \n",
    "                content = \" \".join(paragraphs)\n",
    "                if title and content:  # Include only non-empty content\n",
    "                    articles.append({'Title': title, 'Content': content})\n",
    "\n",
    "        return articles\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data, file_name):\n",
    "    # Remove rows with empty Title or Content\n",
    "    clean_data = [row for row in data if row['Content'].strip()]\n",
    "    if clean_data:\n",
    "        df = pd.DataFrame(clean_data)\n",
    "        df.to_csv(file_name, index=False)\n",
    "        logging.info(f\"Data saved to {file_name}\")\n",
    "    else:\n",
    "        logging.warning(\"No valid data to save!\")\n",
    "\n",
    "# Updated list of URLs from various health and fitness sites\n",
    "urls = [\n",
    "    # Mayo Clinic URLs\n",
    "    \"https://www.mayoclinic.org/healthy-lifestyle/fitness/expert-answers\",\n",
    "    \"https://www.mayoclinic.org/healthy-lifestyle/fitness/in-depth/exercise/art-20045335\",\n",
    "    \"https://www.mayoclinic.org/healthy-lifestyle/nutrition-and-healthy-eating/expert-answers\",\n",
    "    \n",
    "    # WebMD URLs\n",
    "    \"https://www.webmd.com/fitness-exercise/default.htm\",\n",
    "    \"https://www.webmd.com/diet/default.htm\",\n",
    "    \"https://www.webmd.com/healthy-aging/guide/default.htm\",\n",
    "    \n",
    "    # Healthline URLs\n",
    "    \"https://www.healthline.com/fitness-exercise\",\n",
    "    \"https://www.healthline.com/nutrition\",\n",
    "    \"https://www.healthline.com/health\",\n",
    "    \n",
    "    # Verywell Fit URLs\n",
    "    \"https://www.verywellfit.com/\",\n",
    "    \"https://www.verywellfit.com/strength-training-5196435\",\n",
    "    \"https://www.verywellfit.com/nutrition-5196409\",\n",
    "    \n",
    "    # American Heart Association URLs\n",
    "    \"https://www.heart.org/en/healthy-living/fitness\",\n",
    "    \"https://www.heart.org/en/healthy-living/healthy-eating\",\n",
    "    \n",
    "    # NIH URLs\n",
    "    \"https://www.nih.gov/news-events/nih-research-matters\",\n",
    "    \"https://www.nia.nih.gov/news\",\n",
    "    \"https://www.niddk.nih.gov/health-information\",\n",
    "    \n",
    "    # Prevention URLs\n",
    "    \"https://www.prevention.com/\",\n",
    "    \"https://www.prevention.com/fitness/\",\n",
    "    \"https://www.prevention.com/weight-loss/\",\n",
    "    \n",
    "    # Livestrong URLs\n",
    "    \"https://www.livestrong.com/\",\n",
    "    \"https://www.livestrong.com/healthy-living/\",\n",
    "    \"https://www.livestrong.com/nutrition/\",\n",
    "    \n",
    "    # Everyday Health URLs\n",
    "    \"https://www.everydayhealth.com/fitness/\",\n",
    "    \"https://www.everydayhealth.com/diet-nutrition/\",\n",
    "    \"https://www.everydayhealth.com/mental-health/\",\n",
    "    \n",
    "    # Cleveland Clinic URLs\n",
    "    \"https://health.clevelandclinic.org/\",\n",
    "    \"https://health.clevelandclinic.org/category/exercise-fitness/\",\n",
    "    \"https://health.clevelandclinic.org/category/heart-health/\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store all scraped data\n",
    "all_articles = []\n",
    "\n",
    "# Loop through each URL and scrape data\n",
    "for url in urls:\n",
    "    logging.info(f\"Scraping: {url}\")\n",
    "    scraped_data = scrape_article_content(url)\n",
    "    if scraped_data:\n",
    "        all_articles.extend(scraped_data)  # Append scraped data to the main list\n",
    "    # Avoid overloading the server, wait for a brief moment before next request\n",
    "    time.sleep(2)  # Wait 2 seconds between requests\n",
    "\n",
    "# Save all scraped data to a single CSV file\n",
    "save_to_csv(all_articles, \"articles_combined.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.7' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "/opt/homebrew/bin/python3.12 -m pip install ipykernel -U --user --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Setup logging for better error handling\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def scrape_article_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        response.raise_for_status()  # Check for request errors\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find the main content area dynamically\n",
    "        main_content = soup.find('main') or soup.body or soup  # Fallback to body or whole page\n",
    "        articles = []\n",
    "\n",
    "        if main_content:\n",
    "            # Extract headings and associated paragraphs dynamically\n",
    "            headers = main_content.find_all(['h1', 'h2', 'h3', 'h4'])  # Common article section headings\n",
    "\n",
    "            for header in headers:\n",
    "                title = header.get_text(strip=True)  # Extract title\n",
    "                paragraphs = []\n",
    "                \n",
    "                for sibling in header.find_next_siblings():\n",
    "                    if sibling.name in ['h1', 'h2', 'h3', 'h4']:  # Stop at next header\n",
    "                        break\n",
    "                    if sibling.name == 'p':  # Extract paragraph text\n",
    "                        paragraphs.append(sibling.get_text(strip=True))\n",
    "                \n",
    "                content = \" \".join(paragraphs)\n",
    "                if title and content:  # Include only non-empty titles and content\n",
    "                    articles.append({'Title': title, 'Content': content})\n",
    "\n",
    "        return articles\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def save_to_csv(data, file_name):\n",
    "    # Remove duplicate articles and rows with empty content\n",
    "    df = pd.DataFrame(data).drop_duplicates(subset='Title').dropna(subset=['Content'])\n",
    "    if not df.empty:\n",
    "        df.to_csv(file_name, index=False)\n",
    "        logging.info(f\"Data saved to {file_name}\")\n",
    "    else:\n",
    "        logging.warning(\"No valid data to save!\")\n",
    "\n",
    "# List of URLs from health, fitness, nutrition, and wellness websites\n",
    "urls = [\n",
    "    # Mayo Clinic\n",
    "    \"https://www.mayoclinic.org/healthy-lifestyle/fitness/expert-answers\",\n",
    "    \"https://www.mayoclinic.org/healthy-lifestyle/nutrition-and-healthy-eating/expert-answers\",\n",
    "    \"https://www.mayoclinic.org/healthy-lifestyle/exercise\",\n",
    "\n",
    "    # WebMD\n",
    "    \"https://www.webmd.com/fitness-exercise/default.htm\",\n",
    "    \"https://www.webmd.com/diet/default.htm\",\n",
    "    \"https://www.webmd.com/healthy-aging/guide/default.htm\",\n",
    "\n",
    "    # Healthline\n",
    "    \"https://www.healthline.com/fitness-exercise\",\n",
    "    \"https://www.healthline.com/nutrition\",\n",
    "    \"https://www.healthline.com/health\",\n",
    "    \"https://www.healthline.com/health/wellness\",\n",
    "\n",
    "    # Verywell Fit\n",
    "    \"https://www.verywellfit.com/\",\n",
    "    \"https://www.verywellfit.com/strength-training-5196435\",\n",
    "    \"https://www.verywellfit.com/nutrition-5196409\",\n",
    "\n",
    "    # Cleveland Clinic\n",
    "    \"https://health.clevelandclinic.org/\",\n",
    "    \"https://health.clevelandclinic.org/category/exercise-fitness/\",\n",
    "    \"https://health.clevelandclinic.org/category/heart-health/\",\n",
    "\n",
    "    # Prevention\n",
    "    \"https://www.prevention.com/\",\n",
    "    \"https://www.prevention.com/fitness/\",\n",
    "    \"https://www.prevention.com/nutrition/\",\n",
    "    \"https://www.prevention.com/weight-loss/\",\n",
    "\n",
    "    # Livestrong\n",
    "    \"https://www.livestrong.com/\",\n",
    "    \"https://www.livestrong.com/healthy-living/\",\n",
    "    \"https://www.livestrong.com/nutrition/\",\n",
    "\n",
    "    # Everyday Health\n",
    "    \"https://www.everydayhealth.com/fitness/\",\n",
    "    \"https://www.everydayhealth.com/diet-nutrition/\",\n",
    "    \"https://www.everydayhealth.com/mental-health/\",\n",
    "\n",
    "    # NIH\n",
    "    \"https://www.nih.gov/news-events/nih-research-matters\",\n",
    "    \"https://www.nia.nih.gov/news\",\n",
    "    \"https://www.niddk.nih.gov/health-information\",\n",
    "\n",
    "    # Harvard Health\n",
    "    \"https://www.health.harvard.edu/topics/exercise-and-fitness\",\n",
    "    \"https://www.health.harvard.edu/topics/healthy-eating\",\n",
    "    \"https://www.health.harvard.edu/topics/wellness\",\n",
    "\n",
    "    # CDC\n",
    "    \"https://www.cdc.gov/physicalactivity/basics/index.htm\",\n",
    "    \"https://www.cdc.gov/nutrition/index.html\",\n",
    "    \"https://www.cdc.gov/chronicdisease/resources/publications/factsheets/nutrition.htm\",\n",
    "\n",
    "    # WHO\n",
    "    \"https://www.who.int/news-room/fact-sheets/detail/physical-activity\",\n",
    "    \"https://www.who.int/news-room/fact-sheets/detail/healthy-diet\",\n",
    "\n",
    "    # ACSM (American College of Sports Medicine)\n",
    "    \"https://www.acsm.org/blogs\",\n",
    "    \"https://www.acsm.org/read-research\",\n",
    "\n",
    "    # ACE Fitness\n",
    "    \"https://www.acefitness.org/education-and-resources/professional/expert-articles/\",\n",
    "    \"https://www.acefitness.org/education-and-resources/lifestyle/blog/\",\n",
    "    \n",
    "    # Johns Hopkins\n",
    "    \"https://www.hopkinsmedicine.org/health/wellness-and-prevention\",\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store all scraped data\n",
    "all_articles = []\n",
    "\n",
    "# Loop through each URL and scrape data\n",
    "for url in urls:\n",
    "    logging.info(f\"Scraping: {url}\")\n",
    "    scraped_data = scrape_article_content(url)\n",
    "    if scraped_data:\n",
    "        all_articles.extend(scraped_data)  # Append scraped data to the main list\n",
    "    # Avoid overloading the server, wait for a brief moment before next request\n",
    "    time.sleep(2)  # Wait 2 seconds between requests\n",
    "\n",
    "# Save all scraped data to a single CSV file\n",
    "save_to_csv(all_articles, \"enhanced_articles_combined.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
